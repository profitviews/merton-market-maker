{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9e214c3",
      "metadata": {},
      "source": [
        "# Merton Market Maker — Jump Diff Calibration + Online Replay Experiments\n",
        "\n",
        "Workflow:\n",
        "\n",
        "1. Load/resample historical prices\n",
        "2. Run offline Merton MLE calibration\n",
        "3. Replay prices through C++ online calibrator\n",
        "4. Compare fair value vs market and monitor parameter drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6421ee95",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "BASE_URL = \"https://www.bitmex.com/api/v1\"\n",
        "SYMBOL = \"XBTUSDT\"\n",
        "DATA_PATH = os.getenv(\"MERTON_MARKET_MAKER_DATA_PATH\", \"./data/trade/bitmex/XBTUSDT\")\n",
        "CALIBRATION_DAYS = 30\n",
        "BAR_MINUTES = 5\n",
        "REPLAY_BARS = 2000\n",
        "FUNDING_RATE_8H = 0.0001\n",
        "HORIZON_HOURS = 8.0\n",
        "\n",
        "print({\n",
        "    \"symbol\": SYMBOL,\n",
        "    \"data_path\": DATA_PATH,\n",
        "    \"calibration_days\": CALIBRATION_DAYS,\n",
        "    \"bar_minutes\": BAR_MINUTES,\n",
        "    \"replay_bars\": REPLAY_BARS,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89a5d8c1",
      "metadata": {},
      "source": [
        "## Data loading helpers\n",
        "\n",
        "Primary path is local Parquet trades (`MERTON_MARKET_MAKER_DATA_PATH`). Parquet columns: `time` (µs), `side` (B/S), `size`, `price`.\n",
        "\n",
        "Fallback path is BitMEX bucketed OHLC API if local data is unavailable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7defc6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_bitmex_ohlc(\n",
        "    symbol: str = SYMBOL,\n",
        "    bin_size: str = \"1h\",\n",
        "    start_time: str | None = None,\n",
        "    end_time: str | None = None,\n",
        "    count: int = 1000,\n",
        ") -> pd.DataFrame:\n",
        "    params = {\"symbol\": symbol, \"binSize\": bin_size, \"count\": min(count, 1000), \"reverse\": \"true\"}\n",
        "    if start_time:\n",
        "        params[\"startTime\"] = start_time\n",
        "    if end_time:\n",
        "        params[\"endTime\"] = end_time\n",
        "\n",
        "    resp = requests.get(f\"{BASE_URL}/trade/bucketed\", params=params, timeout=10)\n",
        "    resp.raise_for_status()\n",
        "    df = pd.DataFrame(resp.json())\n",
        "    if not df.empty:\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
        "        df = df.set_index(\"timestamp\").sort_index()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56073011",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_parquet_trades(data_path: str, days: int = 14) -> pd.DataFrame:\n",
        "    \"\"\"Load trade Parquet files. Expects columns: time (µs), side (B/S), size, price.\"\"\"\n",
        "    files = sorted([f for f in os.listdir(data_path) if f.endswith(\".parquet\")], reverse=True)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No Parquet files found in {data_path}\")\n",
        "\n",
        "    files = files[:days]\n",
        "    dfs = [pd.read_parquet(os.path.join(data_path, f)) for f in files]\n",
        "    trades = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Parquet format: time (microseconds), side (B/S), size, price\n",
        "    trades[\"time\"] = pd.to_datetime(trades[\"time\"], unit=\"us\", utc=True)\n",
        "    trades = trades.set_index(\"time\").sort_index()\n",
        "    return trades[[\"price\"]].rename(columns={\"price\": \"close\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faebe318",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    trades_raw = load_parquet_trades(DATA_PATH, CALIBRATION_DAYS)\n",
        "    ohlc_calib = pd.DataFrame(\n",
        "        {\"close\": trades_raw[\"close\"].resample(f\"{BAR_MINUTES}min\").last()}\n",
        "    ).dropna()\n",
        "    source_used = f\"parquet:{DATA_PATH}\"\n",
        "except (FileNotFoundError, OSError):\n",
        "    end = datetime.now(timezone.utc)\n",
        "    start = end - timedelta(days=CALIBRATION_DAYS)\n",
        "    ohlc_api = fetch_bitmex_ohlc(\n",
        "        SYMBOL,\n",
        "        f\"{BAR_MINUTES}m\",\n",
        "        start.isoformat(),\n",
        "        end.isoformat(),\n",
        "        count=2000,\n",
        "    )\n",
        "    ohlc_calib = ohlc_api.dropna(subset=[\"close\"])[[\"close\"]]\n",
        "    source_used = \"bitmex_api\"\n",
        "\n",
        "print(f\"Loaded bars={len(ohlc_calib)} source={source_used}\")\n",
        "ohlc_calib.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4568e9",
      "metadata": {},
      "source": [
        "## Offline calibration (MLE)\n",
        "\n",
        "Calibrate annualized Merton parameters from log returns:\n",
        "\n",
        "- `sigma`\n",
        "- `lambda`\n",
        "- `mu_j`\n",
        "- `delta_j`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199c087b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def merton_pdf(\n",
        "    x: np.ndarray,\n",
        "    sigma: float,\n",
        "    lam: float,\n",
        "    mu_j: float,\n",
        "    delta_j: float,\n",
        "    dt: float,\n",
        "    r: float = 0.0,\n",
        "    q: float = 0.0,\n",
        "    n_max: int = 15,\n",
        ") -> np.ndarray:\n",
        "    k = np.exp(mu_j + 0.5 * delta_j**2) - 1\n",
        "    drift = (r - q - lam * k - 0.5 * sigma**2) * dt\n",
        "\n",
        "    pdf = np.zeros_like(x, dtype=float)\n",
        "    for n in range(n_max):\n",
        "        mu_n = drift + n * mu_j\n",
        "        var_n = sigma**2 * dt + n * delta_j**2\n",
        "        if var_n <= 0:\n",
        "            continue\n",
        "        sigma_n = np.sqrt(var_n)\n",
        "        w_n = np.exp(-lam * dt) * (lam * dt) ** n / math.factorial(n)\n",
        "        pdf += w_n * norm.pdf(x, loc=mu_n, scale=sigma_n)\n",
        "\n",
        "    return np.clip(pdf, 1e-300, None)\n",
        "\n",
        "\n",
        "def merton_log_likelihood(params: tuple, returns: np.ndarray, dt: float, r: float = 0.0, q: float = 0.0) -> float:\n",
        "    sigma, lam, mu_j, delta_j = params\n",
        "    if sigma <= 0 or lam < 0 or delta_j <= 0:\n",
        "        return 1e10\n",
        "    pdf = merton_pdf(returns, sigma, lam, mu_j, delta_j, dt, r, q)\n",
        "    nll = -np.sum(np.log(pdf))\n",
        "    return nll if np.isfinite(nll) else 1e10\n",
        "\n",
        "\n",
        "def compute_log_returns(ohlc: pd.DataFrame, price_col: str = \"close\") -> tuple[np.ndarray, float]:\n",
        "    prices = ohlc[price_col].astype(float).values\n",
        "    returns = np.diff(np.log(prices))\n",
        "    if isinstance(ohlc.index, pd.DatetimeIndex) and len(ohlc) > 1:\n",
        "        dt_seconds = ohlc.index.to_series().diff().dt.total_seconds().median()\n",
        "        dt = dt_seconds / (365.25 * 24 * 3600)\n",
        "    else:\n",
        "        dt = 1.0 / (365.25 * 24 * 12)\n",
        "    return returns[~np.isnan(returns)], dt\n",
        "\n",
        "\n",
        "def calibrate_merton(returns: np.ndarray, dt: float, r: float = 0.0, q: float = 0.0) -> dict:\n",
        "    vol_emp = np.std(returns) / np.sqrt(dt) if dt > 0 else 0.5\n",
        "    x0 = [max(vol_emp * 0.8, 0.2), 1.0, -0.02, 0.05]\n",
        "    bounds = [\n",
        "        (0.05, 3.0),\n",
        "        (0.01, 20.0),\n",
        "        (-0.5, 0.5),\n",
        "        (0.01, 1.0),\n",
        "    ]\n",
        "\n",
        "    res = minimize(\n",
        "        lambda p: merton_log_likelihood(p, returns, dt, r, q),\n",
        "        x0,\n",
        "        method=\"L-BFGS-B\",\n",
        "        bounds=bounds,\n",
        "        options={\"maxiter\": 500},\n",
        "    )\n",
        "\n",
        "    sigma, lam, mu_j, delta_j = res.x\n",
        "    return {\n",
        "        \"sigma\": float(sigma),\n",
        "        \"lambda\": float(lam),\n",
        "        \"mu_j\": float(mu_j),\n",
        "        \"delta_j\": float(delta_j),\n",
        "        \"k\": float(np.exp(mu_j + 0.5 * delta_j**2) - 1),\n",
        "        \"nll\": float(res.fun),\n",
        "        \"success\": bool(res.success),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32bf8e1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "returns, dt = compute_log_returns(ohlc_calib)\n",
        "params = calibrate_merton(returns, dt)\n",
        "\n",
        "print(f\"n_returns={len(returns)} dt_years={dt:.8f}\")\n",
        "print(\"Calibrated params:\")\n",
        "for key in (\"sigma\", \"lambda\", \"mu_j\", \"delta_j\", \"k\", \"nll\", \"success\"):\n",
        "    print(f\"  {key}: {params[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93bd3190",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_min, x_max = np.percentile(returns, [0.5, 99.5])\n",
        "x_grid = np.linspace(x_min, x_max, 200)\n",
        "pdf_fit = merton_pdf(\n",
        "    x_grid,\n",
        "    params[\"sigma\"],\n",
        "    params[\"lambda\"],\n",
        "    params[\"mu_j\"],\n",
        "    params[\"delta_j\"],\n",
        "    dt,\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 4))\n",
        "ax.hist(returns, bins=60, density=True, alpha=0.6, label=\"empirical\", edgecolor=\"white\")\n",
        "ax.plot(x_grid, pdf_fit, lw=2, label=\"merton_fit\")\n",
        "ax.set_xlabel(\"log_return\")\n",
        "ax.set_ylabel(\"density\")\n",
        "ax.set_title(\"Merton jump-diffusion fit\")\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.25)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbf3b33",
      "metadata": {},
      "outputs": [],
      "source": [
        "calibration_output = {\n",
        "    \"sigma\": params[\"sigma\"],\n",
        "    \"lambda\": params[\"lambda\"],\n",
        "    \"mu_j\": params[\"mu_j\"],\n",
        "    \"delta_j\": params[\"delta_j\"],\n",
        "}\n",
        "calibration_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c641fd3",
      "metadata": {},
      "source": [
        "## C++ online calibrator replay experiments\n",
        "\n",
        "Replay bars through `merton_online_calibrator` to inspect:\n",
        "\n",
        "- online parameter drift\n",
        "- fair value spread vs market\n",
        "- QuantLib helper divergence (`fair_value_quantlib - fair_value`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1637f53",
      "metadata": {},
      "outputs": [],
      "source": [
        "import merton_online_calibrator as moc\n",
        "\n",
        "def set_lambda(obj, value: float) -> None:\n",
        "    setattr(obj, \"lambda\", float(value))\n",
        "\n",
        "\n",
        "def get_lambda(obj) -> float:\n",
        "    return float(getattr(obj, \"lambda\"))\n",
        "\n",
        "\n",
        "def build_cpp_calibrator(initial_params: dict):\n",
        "    p = moc.MertonParams()\n",
        "    p.sigma = float(initial_params[\"sigma\"])\n",
        "    set_lambda(p, float(initial_params[\"lambda\"]))\n",
        "    p.mu_j = float(initial_params[\"mu_j\"])\n",
        "    p.delta_j = float(initial_params[\"delta_j\"])\n",
        "\n",
        "    cfg = moc.CalibratorConfig()\n",
        "    cfg.window_size = 4096\n",
        "    cfg.min_points_for_update = 512\n",
        "    cfg.update_every_n_returns = 128\n",
        "    cfg.n_max = 15\n",
        "    cfg.coordinate_steps = 3\n",
        "\n",
        "    return moc.OnlineMertonCalibrator(p, cfg)\n",
        "\n",
        "\n",
        "def theoretical_price(\n",
        "    S0: float,\n",
        "    sigma: float,\n",
        "    lam: float,\n",
        "    mu_j: float,\n",
        "    delta_j: float,\n",
        "    q_annual: float,\n",
        "    T_years: float,\n",
        "    r: float = 0.0,\n",
        ") -> float:\n",
        "    \"\"\"Merton risk-neutral expected price over horizon T.\"\"\"\n",
        "    k = math.exp(mu_j + 0.5 * delta_j**2) - 1\n",
        "    drift = r - q_annual - lam * k\n",
        "    return S0 * math.exp(drift * T_years)\n",
        "\n",
        "\n",
        "def funding_rate_annual(rate_per_8h: float) -> float:\n",
        "    \"\"\"Convert BitMEX funding rate (per 8h) to annualized simple carry.\"\"\"\n",
        "    return rate_per_8h * (365.25 * 24 / 8)\n",
        "\n",
        "\n",
        "def funding_annual(rate_per_8h: float) -> float:\n",
        "    \"\"\"Convert 8h funding into annualized compounded carry (runtime default).\"\"\"\n",
        "    return (1.0 + rate_per_8h) ** (365.25 * 3.0) - 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c388104c",
      "metadata": {},
      "outputs": [],
      "source": [
        "replay_df = ohlc_calib.tail(REPLAY_BARS).copy()\n",
        "cal = build_cpp_calibrator(calibration_output)\n",
        "\n",
        "q_annual = funding_annual(FUNDING_RATE_8H)\n",
        "t_years = HORIZON_HOURS / (365.25 * 24.0)\n",
        "\n",
        "rows = []\n",
        "updates = []\n",
        "\n",
        "for ts, row in replay_df.iterrows():\n",
        "    px = float(row[\"close\"])\n",
        "    epoch_us = int(ts.value // 1_000)  # pandas ns -> us\n",
        "\n",
        "    accepted = cal.update_tick(px, epoch_us)\n",
        "    updated = bool(accepted and cal.maybe_update_params())\n",
        "\n",
        "    p_now = cal.params()\n",
        "    lam_now = get_lambda(p_now)\n",
        "\n",
        "    fair_fast = float(cal.fair_value(px, q_annual, t_years, 0.0))\n",
        "    fair_ql = float(cal.fair_value_quantlib(px, q_annual, t_years, 0.0))\n",
        "    fair_py = float(\n",
        "        theoretical_price(\n",
        "            px,\n",
        "            float(p_now.sigma),\n",
        "            lam_now,\n",
        "            float(p_now.mu_j),\n",
        "            float(p_now.delta_j),\n",
        "            q_annual,\n",
        "            t_years,\n",
        "            0.0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"time\": ts,\n",
        "            \"close\": px,\n",
        "            \"fair_fast\": fair_fast,\n",
        "            \"fair_ql\": fair_ql,\n",
        "            \"fair_py\": fair_py,\n",
        "            \"spread_fast_bps\": 1e4 * (fair_fast - px) / px,\n",
        "            \"spread_ql_bps\": 1e4 * (fair_ql - px) / px,\n",
        "            \"spread_py_bps\": 1e4 * (fair_py - px) / px,\n",
        "            \"ql_minus_fast_bps\": 1e4 * (fair_ql - fair_fast) / px,\n",
        "            \"py_minus_fast_bps\": 1e4 * (fair_py - fair_fast) / px,\n",
        "            \"sigma\": float(p_now.sigma),\n",
        "            \"lambda\": lam_now,\n",
        "            \"mu_j\": float(p_now.mu_j),\n",
        "            \"delta_j\": float(p_now.delta_j),\n",
        "            \"updated\": updated,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if updated:\n",
        "        updates.append((ts, float(p_now.sigma), lam_now, float(p_now.mu_j), float(p_now.delta_j)))\n",
        "\n",
        "exp_df = pd.DataFrame(rows).set_index(\"time\")\n",
        "\n",
        "print(f\"replay_bars={len(exp_df)} accepted_samples={cal.sample_count()} updates={len(updates)}\")\n",
        "exp_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2afaea3",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_n = min(500, len(exp_df))\n",
        "plot_df = exp_df.tail(show_n)\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "axes[0].plot(plot_df.index, plot_df[\"close\"], label=\"close\", lw=1.5)\n",
        "axes[0].plot(plot_df.index, plot_df[\"fair_fast\"], label=\"fair_fast\", lw=1.2)\n",
        "axes[0].plot(plot_df.index, plot_df[\"fair_ql\"], label=\"fair_ql\", lw=1.0)\n",
        "axes[0].plot(plot_df.index, plot_df[\"fair_py\"], label=\"fair_py\", lw=1.0, alpha=0.8)\n",
        "axes[0].set_title(\"Market vs theoretical fair values\")\n",
        "axes[0].legend(loc=\"upper left\")\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(plot_df.index, plot_df[\"spread_fast_bps\"], label=\"fast spread bps\")\n",
        "axes[1].plot(plot_df.index, plot_df[\"spread_ql_bps\"], label=\"ql spread bps\")\n",
        "axes[1].plot(plot_df.index, plot_df[\"spread_py_bps\"], label=\"py spread bps\", alpha=0.8)\n",
        "axes[1].axhline(0.0, color=\"black\", lw=1)\n",
        "axes[1].set_title(\"Fair minus market spread (bps)\")\n",
        "axes[1].legend(loc=\"upper left\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "axes[2].plot(plot_df.index, plot_df[\"sigma\"], label=\"sigma\")\n",
        "axes[2].plot(plot_df.index, plot_df[\"lambda\"], label=\"lambda\")\n",
        "axes[2].plot(plot_df.index, plot_df[\"delta_j\"], label=\"delta_j\")\n",
        "axes[2].set_title(\"Online parameter evolution\")\n",
        "axes[2].legend(loc=\"upper left\")\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if updates:\n",
        "    updates_df = pd.DataFrame(\n",
        "        updates,\n",
        "        columns=[\"time\", \"sigma\", \"lambda\", \"mu_j\", \"delta_j\"],\n",
        "    ).set_index(\"time\")\n",
        "    display(updates_df.tail(10))\n",
        "else:\n",
        "    print(\"No updates triggered; consider lower min_points/update_every_n_returns or more replay bars.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "895ee342",
      "metadata": {},
      "source": [
        "### Suggested tuning experiments\n",
        "\n",
        "- responsiveness: lower `min_points_for_update` and `update_every_n_returns` in `build_cpp_calibrator`\n",
        "- stability: increase `window_size`\n",
        "- jump truncation sensitivity: vary `n_max`\n",
        "- carry sensitivity: adjust `FUNDING_RATE_8H` and `HORIZON_HOURS`\n",
        "\n",
        "### Export candidate runtime seed\n",
        "\n",
        "Use the latest row from `exp_df[[\"sigma\", \"lambda\", \"mu_j\", \"delta_j\"]]` as a candidate seed for production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6677c9ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import QuantLib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885f2c6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "BASE_URL = \"https://www.bitmex.com/api/v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58cee518",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_bitmex_ohlc(symbol: str = \"XBTUSDT\", bin_size: str = \"1h\", start_time: str = None, end_time: str = None, count: int = 1000) -> pd.DataFrame:\n",
        "    \"\"\"Fetch bucketed OHLC data from BitMEX. bin_size: '1m', '5m', '1h', '1d'.\"\"\"\n",
        "    params = {\"symbol\": symbol, \"binSize\": bin_size, \"count\": min(count, 1000), \"reverse\": \"true\"}\n",
        "    if start_time:\n",
        "        params[\"startTime\"] = start_time\n",
        "    if end_time:\n",
        "        params[\"endTime\"] = end_time\n",
        "    resp = requests.get(f\"{BASE_URL}/trade/bucketed\", params=params)\n",
        "    resp.raise_for_status()\n",
        "    df = pd.DataFrame(resp.json())\n",
        "    if not df.empty:\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "        df = df.set_index(\"timestamp\").sort_index()\n",
        "    return df\n",
        "\n",
        "# BitMEX trades API hangs with count=1000; use 999\n",
        "MAX_TRADE_COUNT = 999\n",
        "\n",
        "def fetch_bitmex_trades(symbol: str = \"XBTUSDT\", start_time: str = None, end_time: str = None, count: int = 1000) -> pd.DataFrame:\n",
        "    \"\"\"Fetch raw trade data from BitMEX. Paginates automatically for historical ranges.\"\"\"\n",
        "    all_trades = []\n",
        "    end = end_time  # Fetch newest-first, so we paginate backward in time\n",
        "    \n",
        "    while True:\n",
        "        chunk_size = min(count, MAX_TRADE_COUNT)\n",
        "        params = {\"symbol\": symbol, \"count\": chunk_size, \"reverse\": \"true\"}\n",
        "        if end:\n",
        "            params[\"endTime\"] = end\n",
        "        resp = requests.get(f\"{BASE_URL}/trade\", params=params)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        if not data:\n",
        "            break\n",
        "        all_trades = data + all_trades\n",
        "        oldest = data[-1][\"timestamp\"]\n",
        "        if start_time and oldest <= start_time:\n",
        "            break\n",
        "        # When no start_time, stop once we have enough\n",
        "        if len(all_trades) >= count:\n",
        "            all_trades = all_trades[-count:]\n",
        "            break\n",
        "        end = oldest\n",
        "        if len(data) < chunk_size:\n",
        "            break\n",
        "    \n",
        "    df = pd.DataFrame(all_trades)\n",
        "    if not df.empty:\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "        df = df.set_index(\"timestamp\").sort_index()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922f5e17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch OHLC data (1h bars) - last 7 days\n",
        "end = datetime.now(timezone.utc)\n",
        "start = end - timedelta(days=7)\n",
        "ohlc = fetch_bitmex_ohlc(\"XBTUSDT\", \"1h\", start.isoformat(), end.isoformat(), count=200)\n",
        "ohlc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2923ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch raw trades (e.g. last 1000 trades - useful for tick-level analysis)\n",
        "trades = fetch_bitmex_trades(\"XBTUSDT\", count=1100)\n",
        "trades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c4d5ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For longer history: fetch OHLC in chunks (API returns max 1000 rows per request)\n",
        "# Example: 1h bars for ~40 days per request\n",
        "def fetch_bitmex_ohlc_range(symbol: str = \"XBTUSDT\", bin_size: str = \"1h\", \n",
        "                            start: datetime = None, end: datetime = None) -> pd.DataFrame:\n",
        "    \"\"\"Fetch full OHLC range with automatic pagination.\"\"\"\n",
        "    all_data = []\n",
        "    current_end = end\n",
        "    while True:\n",
        "        df = fetch_bitmex_ohlc(symbol, bin_size, \n",
        "                               start_time=start.isoformat() + \"Z\" if start else None,\n",
        "                               end_time=current_end.isoformat() + \"Z\" if current_end else None,\n",
        "                               count=1000)\n",
        "        if df.empty:\n",
        "            break\n",
        "        all_data.append(df)\n",
        "        oldest = df.index.min()\n",
        "        if start and oldest <= start:\n",
        "            break\n",
        "        current_end = oldest - timedelta(seconds=1)\n",
        "    return pd.concat(all_data).drop_duplicates().sort_index() if all_data else pd.DataFrame()\n",
        "\n",
        "# Example: last 3 months of 1h data\n",
        "end_dt = datetime.now(timezone.utc)\n",
        "start_dt = end_dt - timedelta(days=90)\n",
        "# historical_ohlc = fetch_bitmex_ohlc_range(\"XBTUSDT\", \"1h\", start_dt, end_dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb8c4cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "trades"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a4cff7",
      "metadata": {},
      "source": [
        "## Phase A: Offline Calibration (Merton Jump Diffusion)\n",
        "\n",
        "Calibrate $[\\sigma, \\lambda, \\mu_J, \\delta_J]$ from historical returns using MLE.\n",
        "\n",
        "PDF: $f(x) = \\sum_{n=0}^{\\infty} \\frac{e^{-\\lambda dt}(\\lambda dt)^n}{n!} \\cdot \\mathcal{N}(x; \\mu_n, \\sigma_n^2)$\n",
        "\n",
        "Where: $\\mu_n = (r - q - \\lambda k - \\sigma^2/2)dt + n\\mu_J$, $\\sigma_n^2 = \\sigma^2 dt + n\\delta_J^2$, $k = e^{\\mu_J + \\delta_J^2/2} - 1$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ad7945",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def merton_pdf(x: np.ndarray, sigma: float, lam: float, mu_j: float, delta_j: float,\n",
        "               dt: float, r: float = 0.0, q: float = 0.0, n_max: int = 15) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Merton jump diffusion PDF for log-return x over period dt.\n",
        "    Parameters are annualized: sigma (vol), lam (jump intensity), mu_j (mean of ln(J)), delta_j (std of ln(J)).\n",
        "    \"\"\"\n",
        "    k = np.exp(mu_j + 0.5 * delta_j**2) - 1  # E[J-1]\n",
        "    drift = (r - q - lam * k - 0.5 * sigma**2) * dt\n",
        "    \n",
        "    pdf = np.zeros_like(x, dtype=float)\n",
        "    for n in range(n_max):\n",
        "        mu_n = drift + n * mu_j\n",
        "        var_n = sigma**2 * dt + n * delta_j**2\n",
        "        if var_n <= 0:\n",
        "            continue\n",
        "        sigma_n = np.sqrt(var_n)\n",
        "        poisson_weight = np.exp(-lam * dt) * (lam * dt)**n / math.factorial(n)\n",
        "        pdf += poisson_weight * norm.pdf(x, loc=mu_n, scale=sigma_n)\n",
        "    return np.clip(pdf, 1e-300, None)  # avoid log(0)\n",
        "\n",
        "def merton_log_likelihood(params: tuple, returns: np.ndarray, dt: float,\n",
        "                          r: float = 0.0, q: float = 0.0) -> float:\n",
        "    \"\"\"Negative log-likelihood for MLE (minimize this).\"\"\"\n",
        "    sigma, lam, mu_j, delta_j = params\n",
        "    if sigma <= 0 or lam < 0 or delta_j <= 0:\n",
        "        return 1e10\n",
        "    pdf = merton_pdf(returns, sigma, lam, mu_j, delta_j, dt, r, q)\n",
        "    nll = -np.sum(np.log(pdf))\n",
        "    return nll if np.isfinite(nll) else 1e10\n",
        "\n",
        "def compute_log_returns(ohlc: pd.DataFrame, price_col: str = \"close\") -> tuple[np.ndarray, float]:\n",
        "    \"\"\"Compute log returns and dt (in years). Assumes sorted by timestamp.\"\"\"\n",
        "    prices = ohlc[price_col].astype(float).values\n",
        "    returns = np.diff(np.log(prices))\n",
        "    # Infer dt from index (median bar duration)\n",
        "    if isinstance(ohlc.index, pd.DatetimeIndex) and len(ohlc) > 1:\n",
        "        deltas = ohlc.index.to_series().diff().dt.total_seconds()\n",
        "        dt_seconds = deltas.median()\n",
        "        dt = dt_seconds / (365.25 * 24 * 3600)  # years\n",
        "    else:\n",
        "        dt = 1 / (365.25 * 24 * 4)  # default: 15-min bars\n",
        "    return returns[~np.isnan(returns)], dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3096515",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate_merton(returns: np.ndarray, dt: float, r: float = 0.0, q: float = 0.0,\n",
        "                    x0: list = None, bounds: tuple = None) -> dict:\n",
        "    \"\"\"\n",
        "    Calibrate Merton JD parameters via MLE.\n",
        "    Returns dict with sigma, lambda, mu_j, delta_j (annualized).\n",
        "    \"\"\"\n",
        "    if x0 is None:\n",
        "        # Initial guess: sigma from sample std, moderate jumps\n",
        "        vol_emp = np.std(returns) / np.sqrt(dt) if dt > 0 else 0.5\n",
        "        x0 = [max(vol_emp * 0.8, 0.2), 1.0, -0.02, 0.05]\n",
        "    if bounds is None:\n",
        "        bounds = [\n",
        "            (0.05, 3.0),   # sigma\n",
        "            (0.01, 20.0),  # lambda (jumps per year)\n",
        "            (-0.5, 0.5),   # mu_j\n",
        "            (0.01, 1.0),   # delta_j\n",
        "        ]\n",
        "    \n",
        "    def neg_ll(p):\n",
        "        return merton_log_likelihood(p, returns, dt, r, q)\n",
        "    \n",
        "    res = minimize(neg_ll, x0, method=\"L-BFGS-B\", bounds=bounds,\n",
        "                   options={\"maxiter\": 500})\n",
        "    \n",
        "    sigma, lam, mu_j, delta_j = res.x\n",
        "    k = np.exp(mu_j + 0.5 * delta_j**2) - 1\n",
        "    return {\n",
        "        \"sigma\": sigma,\n",
        "        \"lambda\": lam,\n",
        "        \"mu_j\": mu_j,\n",
        "        \"delta_j\": delta_j,\n",
        "        \"k\": k,\n",
        "        \"nll\": res.fun,\n",
        "        \"success\": res.success,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87659077",
      "metadata": {},
      "source": [
        "### Calibration data (preferred): Parquet trades\n",
        "\n",
        "Preferred source is local Parquet files. Format: columns `time` (µs), `side` (B/S), `size`, `price`.\n",
        "\n",
        "BitMEX API cells above are still useful for ad-hoc checks, but the calibration and replay experiments below should use the local Parquet path whenever available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f772d8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trades from local Parquet files (columns: time µs, side, size, price)\n",
        "import os\n",
        "\n",
        "DATA_PATH = os.getenv(\"MERTON_MARKET_MAKER_DATA_PATH\", \"./data/trade/bitmex/XBTUSDT\")\n",
        "CALIBRATION_DAYS = 30  # Number of most recent days to load (increase for more history)\n",
        "BAR_MINUTES = 5\n",
        "\n",
        "\n",
        "def load_parquet_trades(data_path: str, days: int = 14) -> pd.DataFrame:\n",
        "    \"\"\"Load trade Parquet files. Expects columns: time (µs), side (B/S), size, price.\"\"\"\n",
        "    files = sorted([f for f in os.listdir(data_path) if f.endswith(\".parquet\")], reverse=True)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No Parquet files found in {data_path}\")\n",
        "    files = files[:days]\n",
        "    dfs = [pd.read_parquet(os.path.join(data_path, f)) for f in files]\n",
        "    trades = pd.concat(dfs, ignore_index=True)\n",
        "    trades[\"time\"] = pd.to_datetime(trades[\"time\"], unit=\"us\", utc=True)\n",
        "    trades = trades.set_index(\"time\").sort_index()\n",
        "    return trades[[\"price\"]].rename(columns={\"price\": \"close\"})\n",
        "\n",
        "\n",
        "# Load trades and resample to OHLC-like bars (close = last price per interval)\n",
        "# Falls back to BitMEX API if Parquet path missing\n",
        "try:\n",
        "    trades_raw = load_parquet_trades(DATA_PATH, CALIBRATION_DAYS)\n",
        "    ohlc_calib = pd.DataFrame({\n",
        "        \"close\": trades_raw[\"close\"].resample(f\"{BAR_MINUTES}min\").last()\n",
        "    }).dropna()\n",
        "    print(f\"Loaded {len(ohlc_calib)} bars from Parquet ({DATA_PATH})\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Parquet not found: {e}. Using BitMEX API.\")\n",
        "    end = datetime.now(timezone.utc)\n",
        "    start = end - timedelta(days=CALIBRATION_DAYS)\n",
        "    ohlc_calib = fetch_bitmex_ohlc(\"XBTUSDT\", f\"{BAR_MINUTES}m\", start.isoformat(), end.isoformat(), count=2000)\n",
        "    ohlc_calib = ohlc_calib.dropna(subset=[\"close\"])[[\"close\"]]\n",
        "ohlc_calib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253dfd6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Calibration data loaded above: Parquet with API fallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599c0505",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute log returns and calibrate\n",
        "returns, dt = compute_log_returns(ohlc_calib)\n",
        "print(f\"Log returns: n={len(returns)}, dt={dt:.6f} years ({dt*365.25*24*60:.1f} min bars)\")\n",
        "\n",
        "params = calibrate_merton(returns, dt)\n",
        "print(f\"\\nCalibrated parameters (annualized):\")\n",
        "print(f\"  σ (sigma)   = {params['sigma']:.4f}\")\n",
        "print(f\"  λ (lambda)  = {params['lambda']:.4f}  (jumps/year)\")\n",
        "print(f\"  μ_J (mu_j)  = {params['mu_j']:.4f}\")\n",
        "print(f\"  δ_J (delta_j)= {params['delta_j']:.4f}\")\n",
        "print(f\"  k = E[J-1]  = {params['k']:.6f}\")\n",
        "print(f\"  NLL         = {params['nll']:.2f} (success={params['success']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbdeaf8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize: empirical vs fitted PDF\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_min, x_max = np.percentile(returns, [0.5, 99.5])\n",
        "x_grid = np.linspace(x_min, x_max, 200)\n",
        "pdf_fit = merton_pdf(x_grid, params[\"sigma\"], params[\"lambda\"], \n",
        "                     params[\"mu_j\"], params[\"delta_j\"], dt)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 4))\n",
        "ax.hist(returns, bins=60, density=True, alpha=0.6, label=\"Empirical\", color=\"steelblue\", edgecolor=\"white\")\n",
        "ax.plot(x_grid, pdf_fit, \"r-\", lw=2, label=\"Merton JD fit\")\n",
        "ax.set_xlabel(\"Log return\")\n",
        "ax.set_ylabel(\"Density\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Merton Jump Diffusion Calibration\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b116e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters for QuantLib / C++ engine (feed to algo)\n",
        "calibration_output = {\n",
        "    \"sigma\": params[\"sigma\"],\n",
        "    \"lambda\": params[\"lambda\"],\n",
        "    \"mu_j\": params[\"mu_j\"],\n",
        "    \"delta_j\": params[\"delta_j\"],\n",
        "}\n",
        "print(\"Calibration output (for algo):\")\n",
        "for k, v in calibration_output.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b2e83b8",
      "metadata": {},
      "source": [
        "## Phase B: C++ online calibrator experimentation\n",
        "\n",
        "This section replays historical bars through `merton_online_calibrator` and helps you test the production setup.\n",
        "\n",
        "What you can experiment with:\n",
        "\n",
        "- online update cadence (`update_every_n_returns`)\n",
        "- window length (`window_size`)\n",
        "- MLE truncation/config (`n_max`, `coordinate_steps`)\n",
        "- divergence between fast fair value and QuantLib helper fair value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2a73a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the compiled C++ module and define helpers.\n",
        "import merton_online_calibrator as moc\n",
        "print(\"Loaded C++ module:\", moc.__file__)\n",
        "\n",
        "\n",
        "def _set_lambda(obj, value: float) -> None:\n",
        "    # \"lambda\" is a Python keyword, so use setattr/getattr.\n",
        "    setattr(obj, \"lambda\", float(value))\n",
        "\n",
        "\n",
        "def _get_lambda(obj) -> float:\n",
        "    return float(getattr(obj, \"lambda\"))\n",
        "\n",
        "\n",
        "def make_cpp_calibrator(\n",
        "    initial_params: dict,\n",
        "    *,\n",
        "    window_size: int = 4096,\n",
        "    min_points_for_update: int = 512,\n",
        "    update_every_n_returns: int = 128,\n",
        "    n_max: int = 15,\n",
        "    coordinate_steps: int = 3,\n",
        "):\n",
        "    p = moc.MertonParams()\n",
        "    p.sigma = float(initial_params[\"sigma\"])\n",
        "    _set_lambda(p, float(initial_params[\"lambda\"]))\n",
        "    p.mu_j = float(initial_params[\"mu_j\"])\n",
        "    p.delta_j = float(initial_params[\"delta_j\"])\n",
        "\n",
        "    cfg = moc.CalibratorConfig()\n",
        "    cfg.window_size = int(window_size)\n",
        "    cfg.min_points_for_update = int(min_points_for_update)\n",
        "    cfg.update_every_n_returns = int(update_every_n_returns)\n",
        "    cfg.n_max = int(n_max)\n",
        "    cfg.coordinate_steps = int(coordinate_steps)\n",
        "\n",
        "    return moc.OnlineMertonCalibrator(p, cfg)\n",
        "\n",
        "\n",
        "def funding_annual(funding_rate_8h: float) -> float:\n",
        "    # Convert 8h discrete funding into annualized carry.\n",
        "    return (1.0 + funding_rate_8h) ** (365.25 * 3.0) - 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f149cfad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replay the most recent bars into the C++ online calibrator.\n",
        "# Tune these values to experiment with runtime behavior.\n",
        "REPLAY_BARS = 2000\n",
        "FUNDING_RATE_8H = 0.0001\n",
        "HORIZON_HOURS = 8.0\n",
        "\n",
        "replay_df = ohlc_calib.tail(REPLAY_BARS).copy()\n",
        "cal = make_cpp_calibrator(\n",
        "    calibration_output,\n",
        "    window_size=4096,\n",
        "    min_points_for_update=512,\n",
        "    update_every_n_returns=128,\n",
        "    n_max=15,\n",
        "    coordinate_steps=3,\n",
        ")\n",
        "\n",
        "q_annual = funding_annual(FUNDING_RATE_8H)\n",
        "t_years = HORIZON_HOURS / (365.25 * 24.0)\n",
        "\n",
        "records = []\n",
        "updates = []\n",
        "\n",
        "for ts, row in replay_df.iterrows():\n",
        "    px = float(row[\"close\"])\n",
        "    epoch_us = int(ts.value // 1_000)  # pandas ns -> us\n",
        "\n",
        "    accepted = cal.update_tick(px, epoch_us)\n",
        "    updated = bool(accepted and cal.maybe_update_params())\n",
        "\n",
        "    p_now = cal.params()\n",
        "    lam_now = _get_lambda(p_now)\n",
        "\n",
        "    fair_fast = float(cal.fair_value(px, q_annual, t_years, 0.0))\n",
        "    fair_ql = float(cal.fair_value_quantlib(px, q_annual, t_years, 0.0))\n",
        "\n",
        "    records.append(\n",
        "        {\n",
        "            \"time\": ts,\n",
        "            \"close\": px,\n",
        "            \"fair_fast\": fair_fast,\n",
        "            \"fair_ql\": fair_ql,\n",
        "            \"spread_fast_bps\": 1e4 * (fair_fast - px) / px,\n",
        "            \"spread_ql_bps\": 1e4 * (fair_ql - px) / px,\n",
        "            \"ql_minus_fast_bps\": 1e4 * (fair_ql - fair_fast) / px,\n",
        "            \"sigma\": float(p_now.sigma),\n",
        "            \"lambda\": lam_now,\n",
        "            \"mu_j\": float(p_now.mu_j),\n",
        "            \"delta_j\": float(p_now.delta_j),\n",
        "            \"updated\": updated,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if updated:\n",
        "        updates.append((ts, float(p_now.sigma), lam_now, float(p_now.mu_j), float(p_now.delta_j)))\n",
        "\n",
        "exp_df = pd.DataFrame(records).set_index(\"time\")\n",
        "\n",
        "print(f\"Replay bars: {len(exp_df)}\")\n",
        "print(f\"Accepted samples: {cal.sample_count()}\")\n",
        "print(f\"Online updates triggered: {len(updates)}\")\n",
        "print(\"\\nFinal online params:\")\n",
        "print(exp_df[[\"sigma\", \"lambda\", \"mu_j\", \"delta_j\"]].tail(1).to_string())\n",
        "\n",
        "exp_df.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73e8301",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual diagnostics for experimentation.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "show_n = min(500, len(exp_df))\n",
        "plot_df = exp_df.tail(show_n)\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "axes[0].plot(plot_df.index, plot_df[\"close\"], label=\"close\", lw=1.5)\n",
        "axes[0].plot(plot_df.index, plot_df[\"fair_fast\"], label=\"fair_fast\", lw=1.2)\n",
        "axes[0].plot(plot_df.index, plot_df[\"fair_ql\"], label=\"fair_ql\", lw=1.0, alpha=0.9)\n",
        "axes[0].set_title(\"Market close vs theoretical fair values\")\n",
        "axes[0].legend(loc=\"upper left\")\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(plot_df.index, plot_df[\"spread_fast_bps\"], label=\"fast spread (bps)\")\n",
        "axes[1].plot(plot_df.index, plot_df[\"spread_ql_bps\"], label=\"ql spread (bps)\", alpha=0.9)\n",
        "axes[1].axhline(0.0, color=\"black\", lw=1)\n",
        "axes[1].set_title(\"Fair - market spread in bps\")\n",
        "axes[1].legend(loc=\"upper left\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "axes[2].plot(plot_df.index, plot_df[\"sigma\"], label=\"sigma\")\n",
        "axes[2].plot(plot_df.index, plot_df[\"lambda\"], label=\"lambda\")\n",
        "axes[2].plot(plot_df.index, plot_df[\"delta_j\"], label=\"delta_j\")\n",
        "axes[2].set_title(\"Online parameter evolution\")\n",
        "axes[2].legend(loc=\"upper left\")\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if updates:\n",
        "    updates_df = pd.DataFrame(\n",
        "        updates,\n",
        "        columns=[\"time\", \"sigma\", \"lambda\", \"mu_j\", \"delta_j\"],\n",
        "    ).set_index(\"time\")\n",
        "    display(updates_df.tail(10))\n",
        "else:\n",
        "    print(\"No online parameter updates were triggered. Try increasing REPLAY_BARS or reducing min_points_for_update.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61235c7",
      "metadata": {},
      "source": [
        "### Suggested experiments\n",
        "\n",
        "1. **Stability vs responsiveness**\n",
        "   - lower `min_points_for_update` and `update_every_n_returns` for faster adaptation\n",
        "   - increase them for smoother, less noisy parameters\n",
        "\n",
        "2. **Likelihood truncation sensitivity**\n",
        "   - vary `n_max` (e.g. `10`, `15`, `20`) and compare spread distributions\n",
        "\n",
        "3. **QuantLib helper divergence**\n",
        "   - monitor `ql_minus_fast_bps`; it should usually stay small\n",
        "   - spikes can indicate carry assumptions (`FUNDING_RATE_8H`, `HORIZON_HOURS`) need adjustment\n",
        "\n",
        "4. **Deployment seed selection**\n",
        "   - use final rows from `exp_df[[\"sigma\", \"lambda\", \"mu_j\", \"delta_j\"]]` as candidate runtime seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "504844ab",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "merton-market-maker",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
